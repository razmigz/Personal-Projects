---
title: "Housing"
author: "Razmig Zeitounian"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note: the data set is available from https://www.kaggle.com/harlfoxem/housesalesprediction

```{r, echo = FALSE, message = FALSE}
#load in libraries
library(readr)
library(tidyverse)
library(Hmisc)
library(corrplot)
library(GGally)
library(FactoMineR)
library(olsrr)
library(caret)
library(glmnet)
library(MASS)
library(leaps)
library(car)
library(boot)
library(gridExtra)

df <- read_csv("housePrices/kc_house_data.csv")

#drop id and date col since we wont use those
df <- df %>% 
  dplyr::select(-c(id, date))

#data from 2015; use that as "current" year in order to create an "effective years old" variable; this will be more helpful
#in determining a good age since renovations make a house feel newer
years_old <- function(built = df$yr_built, renovated = df$yr_renovated){
  #consider no renovation and most recent renovation case to create an effective years old variable for each house
  if(renovated == 0){
    return(2015-built)
  } else {
  new_age = 2015 - renovated - built  
  return(new_age)
  }
}

#now drop yr built and renovated since we have a better variable to use for comparison
df <- cbind(df, years_old()) %>% 
  dplyr::select(-c(yr_built, yr_renovated)) %>% 
  rename(effective_age = `years_old()`)

#treat zipcode, waterfront, view, condition as factors
df$zipcode <- as.factor(df$zipcode)
df$waterfront <- as.factor(df$waterfront)
df$view <- as.factor(df$view)
df$condition <- as.factor(df$view)

#get rid of sqft_living and sqft_lot since we have the more modern version (2015) so we have redundancy
df <- df %>% 
  dplyr::select(-c(sqft_living, sqft_lot))

#see how many zip codes we have total and how many properties from each
df %>% 
  group_by(zipcode) %>% 
  count()

glimpse(df)
```

```{r, plots}
#limit to 1 mil dollars since it is a minority of the data that heavily skews the hist
df %>% 
  filter(price > 1000000) %>%
  count() / nrow(df)

limit.df <- df %>% 
  filter(price < 1000000)
 
price.hist <- ggplot(limit.df, aes(price)) + 
  geom_histogram(col = "black", fill = "pink", binwidth = 10000) +
  ylab("Count") +
  xlab ("Price (in USD)") +
  ggtitle("Distribution of Housing Prices in King County") +labs(subtitle = "*Prices are capped at 1 million dollars for better visibility")

#get rid of scientific notation in x axis
price.hist + 
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 

#suggests many outliers
boxplot <- ggplot(df, aes(price)) + 
  geom_boxplot(fill = "orange", color = "purple") +
  coord_flip() + 
  ggtitle("Boxplot of Housing Prices")
boxplot

#create a new df to remove outliers
outliers <- boxplot.stats(df$price)$out %>% 
  as_tibble()

df.no.outliers <- df[-outliers$value, ]
```

```{r Univariate outliers}
#create a new df to remove outliers
outliers <- boxplot.stats(df$price)$out %>% 
  as_tibble()

#check to see how much data we would remove if we would remove outliers under this method
length(outliers$value)/nrow(df)

#new df w/ about 5% of data removed
df.no.outliers <- df[-outliers$value, ]
```

When using outliers via boxplot method, we see that about 5% of the data are outliers. We will have to keep outliers in consideration as we go on.

or

When considering $price$ only, we find that we have some outliers, and they make up about 5% of the data. We will look again at outliers later to see if we should keep them or drop them when creating a model.

```{r Quick Plots}

#check for linear relationship w/ some variables on price; limit to >0 and <= 8 bedrooms due to the others having few data
df.max8 <- df %>% 
  filter(bedrooms <= 8 & bedrooms > 0)

#predefine bathroom as factor so it looks nicer on the legend
n_bathrooms <- as.factor(df.max8$bathrooms)
df.max8 <- cbind(df.max8, n_bathrooms)

p1 <- ggplot(df.max8, aes(x = sqft_living15, y = price)) +
  geom_point(aes(color = n_bathrooms)) + 
  geom_smooth(method = "lm") +
  ggtitle("Relationship between some sqft_living variables and price (in USD), by number of bathrooms")

p1

p1 +
  facet_grid(bedrooms~.) +
  ggtitle("Relationship between some sqft_living variables and price (in USD), by number of bathrooms and bedrooms")
```

We can see a linear trend in the data with some possible outliers that may be skewing the regression model. Having more bathrooms is correlated with a higher price.


```{r Correlation, message=FALSE}
#https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/

response.df <- df %>% 
  dplyr::select(price)

#only consider continuous vars for linear regression
check.corr <- df %>% 
  dplyr::select(-c(zipcode, waterfront, view, condition))

#create a correlation matrix to see which (if any) variables are correlated and plot
mydata.cor = cor(check.corr, method = c("spearman"))
mydata.rcorr = rcorr(as.matrix(check.corr))
#pick whichever one is better
corrplot(mydata.cor, method = "number")
corrplot.mixed(mydata.cor, lower.col = "black", number.cex = .8)
```

From the correlation matrix and plot, we can see that price is moderately correlated with $sqft_{living_{15}}$, $grade$, $sqfoot_{above}$ and $bathrooms$. Specifically, having more of these explanatory variables can lead to a higher price. We should, however, check for multicollinearity (it seems like $sqft_{above}$ and $bathrooms$ are highly correlated for example) and use model selection criteria to see the best linear model we can create. 

We must keep in mind that a regression model must satisfy the following assumptions:
  All samples are independent of one another and are random and identically distributed
  The residuals have mean 0 and are scattered randomly (that is, there is no pattern to the residuals) under a normal        distribution
  

```{r Regression Models}
#create a full model that uses all numeric vars
full.model <- lm(price ~ ., data = check.corr)

#visualize fitted points; we can see it fans
ggplot(df, aes(x=price, y=full.model$fitted.values)) +
  geom_point(color = "pink") +
  labs(x="Price", y="Fitted Prices under a full model Linear Regression", title = "Fitted Prices vs. Actual Prices")

step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)

#suggests many possible predictors, and sqft_lot15 is the most insignificant (highest p-val & lowest t val)
summary(step.model)

#check diagnostics of linear model - it seems like our model violates some assumptions; it has fanning (in residuals vs fitted) and non-normal residuals (from qqplot)
plot(step.model)

my.model <- lm(price ~ bedrooms + bathrooms + floors + sqft_above + lat, data = check.corr)
plot(my.model, main = "Full model diagnostics under a full model with original data")

#lets try some transformations to try to meet diagnostics
log.p <- log(df$price)

#still not good
full.model.t <- lm(log.p ~ ., data = df)
plot(full.model.t, col = "blue")

#try w/ removed outliers
Q <- quantile(df$price, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(df$price)
up <-  Q[2]+1.5*iqr # Upper Range  
low <- Q[1]-1.5*iqr # Lower Range; does not make sense in this context

#df with outliers defined using quantiles removed
rm.out <- df %>% 
  filter(price <= up)

#errors look mostly random here, but normality assumption is not met, but plot still looks good
mod1 <- lm(price ~ ., data = rm.out)
summary(mod1)
plot(mod1)
#this is a big improvement compared to before!; note that this model holds only when removing outliers (prices > up=1129575)
#but the qqplot suggests normality is not met; residuals look randomly scattered around 0
lm.res.fit <- ggplot(rm.out, aes(y=mod1$residuals, x=mod1$fitted.values)) +
  geom_point(color = "navy") + 
  labs(x="Fitted Values with Linear Model", y="Residuals with Model1", title = "Residuals vs. Fitted Values under Linear Model (Outliers Removed)") +
  geom_smooth(method = "lm")
lm.res.fit

#this made it worse
mod2 <- lm(log(price) ~ ., data = rm.out)
plot(mod2)

#do boxcox; suggests to not transform since lambda is ~0
boxCox(full.model, family="yjPower", plotit = TRUE)
```

Since we cannot get a model that meets diagnostics, even after transfomrations, we will create a GLM.


We will try a GLM to see its performance.

```{r GLM Model}
#https://www.educba.com/glm-in-r/
glm.mod <- glm(price ~ ., data = check.corr)
summary(glm.mod)

ggplot(df, aes(price, glm.mod$fitted.values)) +
  geom_point(color = "green") +
  ggtitle("Fitted GLM values vs Actual Values") +
  labs(y = "Fitted Price Values", x = "Price Values")

df.rescale <- df %>% 
  mutate_if(is.numeric, funs(as.numeric(scale(.))))

#under this model, we see that floors is insignificant; dont include zipcode since it has too many factors (30) and condition has NA's
glm.mod2 <- glm(price ~ . -zipcode -condition, data = df.rescale)
summary(glm.mod2) #AIC: 35756

#try dropping variables to compare AIC values; start with floors since it has lowest t-val
glm.mod3 <- glm(price ~ . - zipcode -condition -floors, data = df.rescale)
summary(glm.mod3) #AIC: 35754

glm.mod4 <- glm(price ~ .- zipcode -condition -floors -sqft_lot15, data = df.rescale)
summary(glm.mod4) #AIC: 35772; went up compared to 3

#we can see this model performed poorly here; the linear regression we had did better than this
ggplot(df.rescale, aes(price, glm.mod3$fitted.values)) +
  geom_point(color = "seagreen2")

#try removing outliers again
Q.r <- quantile(df.rescale$price, probs=c(.25, .75), na.rm = FALSE)
iqr.r <- IQR(df.rescale$price)
up.r <-  Q[2]+1.5*iqr.r # Upper Range  
low.r <- Q[1]-1.5*iqr.r # Lower Range; does not make sense in this context

#df with outliers defined using quantiles removed
rm.rescale.out <- df.rescale %>% 
  filter(price <= up.r)

glm.r1 <- glm(price ~ . -zipcode -condition, data = rm.rescale.out)
summary(glm.r1) #AIC: 11097

glm.r2 <- glm(price ~ . -zipcode -condition -floors, data = rm.rescale.out)
summary(glm.r2) #AIC: 11262, but is easier to interpet

glm.r3 <- glm(price ~ . -zipcode -condition -floors -long, data = rm.rescale.out)
summary(glm.r3) #AIC: 11263; means model2 is better if using AIC criterion

#compare residuals to fitted values; it looks like we have a bit of fanning here, so the linear model seems more appropriate
glm.res.fit <- ggplot(rm.rescale.out, aes(y=glm.r2$residuals, x=glm.r2$fitted.values)) +
  geom_point(color = "aquamarine") +
  geom_hline(yintercept = 0) +
  labs(x="Fitted Values under GLM", y = "Residuals under GLM", title = "GLM Residuals vs. Fitted Values")
glm.res.fit
```

Let's compare the linear model residuals vs fitted values plto with the GLM, graphically.

```{r compare models}
grid.arrange(lm.res.fit, glm.res.fit)
```

Since the GLM seems to have a fanning issue, we should keep the linear model instead as a "better" model.


Due to high imbalance, we must assign weights.

# References
https://www.r-bloggers.com/outlier-detection-and-treatment-with-r/
http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/
https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r
https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/
https://rstatisticsblog.com/data-science-in-action/machine-learning/binary-logistic-regression-with-r/
https://www.r-bloggers.com/dealing-with-unbalanced-data-in-machine-learning/
https://www.statmethods.net/advstats/bootstrapping.html



