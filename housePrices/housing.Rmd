---
title: "housing"
author: "Razmig Zeitounian"
date: "7/20/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note: the data set is available from https://www.kaggle.com/harlfoxem/housesalesprediction

```{r, echo = FALSE, message = FALSE}
#load in libraries
library(readr)
library(tidyverse)
library(Hmisc)
library(corrplot)
library(GGally)
library(FactoMineR)
library(olsrr)
library(caret)
library(glmnet)
library(MASS)
library(leaps)

df <- read_csv("housePrices/kc_house_data.csv")

#drop id and date col since we wont use those
df <- df %>% 
  dplyr::select(-c(id, date))

#data from 2015; use that as "current" year in order to create an effective years old variable; this will be more helpful
#in determining a good age since renovations make a house feel newer
years_old <- function(built = df$yr_built, renovated = df$yr_renovated){
  #consider no renovation and most recent renovation case to create an effective years old variable for each house
  if(renovated == 0){
    return(2015-built)
  } else {
  new_age = 2015 - renovated - built  
  return(new_age)
  }
}

#now drop yr built and renovated since we have a better variable to use for comparison
df <- cbind(df, years_old()) %>% 
  dplyr::select(-c(yr_built, yr_renovated)) %>% 
  rename(effective_age = `years_old()`)

#treat zipcode, waterfront, view, condition as factors
df$zipcode <- as.factor(df$zipcode)
df$waterfront <- as.factor(df$waterfront)
df$view <- as.factor(df$view)
df$condition <- as.factor(df$view)

#get rid of sqft_living and sqft_lot since we have the more modern version (2015) so we have redundancy
df <- df %>% 
  dplyr::select(-c(sqft_living, sqft_lot))

glimpse(df)

# df %>% filter(condition == 1) %>% pull(price) %>% prettyNum(.) %>% top_n(20)
```

```{r, plots}
#limit to 1 mil dollars since it is a minority of the data that heavily skews the hist
df %>% 
  filter(price > 1000000) %>%
  count() / nrow(df)

limit.df <- df %>% 
  filter(price < 1000000)
 
price.hist <- ggplot(limit.df, aes(price)) + 
  geom_histogram(col = "black", fill = "pink", binwidth = 10000) +
  ylab("Count") +
  xlab ("Price (in USD)")
  ggtitle("Distribution of Housing Prices in King County") 

#get rid of scientific notation in x axis
price.hist + scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 

#suggests many outliers
boxplot <- ggplot(df, aes(price)) + 
  geom_boxplot(fill = "orange", color = "purple") +
  coord_flip() + 
  ggtitle("Boxplot of Housing Prices")
boxplot

#create a new df to remove outliers
outliers <- boxplot.stats(df$price)$out %>% 
  as_tibble()

df.no.outliers <- df[-outliers$value, ]
```

```{r Univariate outliers}
#create a new df to remove outliers
outliers <- boxplot.stats(df$price)$out %>% 
  as_tibble()

#check to see how much data we would remove if we would remove outliers under this method
length(outliers$value)/nrow(df)

#new df w/ about 5% of data removed
df.no.outliers <- df[-outliers$value, ]

#check for linear relationship w/ some variables on price
ggplot(df.no.outliers, aes(x= bedrooms + bathrooms + sqft_living15, y = price)) +
  geom_smooth(method = "lm") +
  ggtitle("Relationship between some predictor variables and price (in USD)") 

#do we need to scale?
```

When considering $price$ only, we find that we have some outliers, and they make up about 5% of the daat. We will look again at outliers later to see if we should keep them or drop them when creating a model.


```{r Correlation, message=FALSE}
#https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/

response.df <- df %>% 
  dplyr::select(price)

#only consider continuous vars for linear regression
check.corr <- df %>% 
  dplyr::select(-c(zipcode, waterfront, view, condition))

#create a correlation matrix to see which (if any) variables are correlated and plot
mydata.cor = cor(check.corr, method = c("spearman"))
mydata.rcorr = rcorr(as.matrix(check.corr))
#pick whichever one is better
corrplot(mydata.cor, method = "number")
corrplot.mixed(mydata.cor, lower.col = "black", number.cex = .8)
```

From the correlation matrix and plot, we can see that price is moderately correlated with $sqft_{living_{15}}$, $grade$, $sqfoot_{above}$ and $bathrooms$. Specifically, having more of these explanatory variables can lead to a higher price. We should, however, check for multicollinearity (it seems like $sqft_{above}$ and $bathrooms$ are highly correlated for example) and use model selection criteria to see the best linear model we can create. 

```{r Regression Models}
full.model <- lm(price ~ ., data = df)

step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
#suggests many possible predictors; note that some zip codes are significant while others are not
summary(step.model)

step.aic <- step.model$anova$AIC
#check diagnostics of linear model - it seems like our model violates some assumptions; it has fanning (in residuals vs fitted) and non-normal residuals (from qqplot)
plot(step.model)

my.model <- lm(price ~ bedrooms + bathrooms + floors + sqft_above + lat, data = df)
plot(my.model)

#let's try removing outliers first
#########################


#lets try some transformations
log.p <- log(df$price)

#still not good
full.model.t <- lm(log.p ~ ., data = df)
plot(full.model.t)
```


```{r ML Setup}
#use 70% as training
index = sample(1:nrow(df), 0.7*nrow(df)) 
train = df[index,] # Create the training data 
test = df[-index,] # Create the test data
pre_proc_val <- preProcess(train[,2:18], method = c("center", "scale"))

train[, 2:18] = predict(pre_proc_val, train[,2:18])
test[, 2:18] = predict(pre_proc_val, test[,2:18])

#check lin model of training
training.preds <- dplyr::select(df, -price) %>% 
  as_tibble()

#set up a linear model on all predictor variables
training.model.full <- lm(price ~ ., data = train)
summary(training.model.full)

#Step 1 - create the evaluation metrics function
eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 2))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 2))
    cat("Adjusted R^2 is:",adj_r2, "\n") #Adjusted R-squared
    cat("RMSE is:", as.character(round(sqrt(sum(resids2)/N), 2))) #RMSE
}

# Step 2 - predicting and evaluating the model on train data
predictions = predict(training.model.full, newdata = train)
eval_metrics(training.model.full, train, predictions, target = 'price')

# Step 3 - predicting and evaluating the model on test data
predictions = predict(training.model.full, newdata = test)

eval_metrics(training.model.full, test, predictions, target = 'price')
```

We can see many predictors may be significant predictors of price (p-val < 0.05 for many variables). Our adjusted $R^2$ is also fairly high (about 81%). This means that about 81% of variation in price is accounted for by our predictor variables, which means we have a fairly strong correlation beween $price$ and the predictor variables. 


```{r}
#set up dummy vars for predictors
dummies <- dummyVars(price ~ ., data = df)

train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x_vars <- model.matrix(price~. , df)[,-1]
y_var <- df$price
lambda_seq <- 10^seq(2, -2, by = -.1)
 
# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]
 
cv_output <- cv.glmnet(x_vars[train,], y_var[train],
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5)
 
# identifying best lamda
best_lam <- cv_output$lambda.min
best_lam

# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)
pred <- predict(lasso_best, s = best_lam, newx = x_vars[x_test,])



#lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambda_seq, standardize = TRUE, nfolds = 5)

# Best 
# lambda_best <- lasso_reg$lambda.min 
# lambda_best
```

```{r Logistic Regression for Classification}
#lets see if we can predict if a house is waterfront or not
#set up for a logit model

#view prior counts
table(df$waterfront)

#now as percents
(table.percentages <- table(df$waterfront)/sum(table(df$waterfront)))

#notice that a strong majority of the data is NOT waterfront; i.e., it is imbalanced
#this means we should use all waterfronts and sample the non waterfronts (see: https://stats.stackexchange.com/questions/164693/adding-weights-to-logistic-regression-for-imbalanced-data)

#replace for clarity in graph
df$waterfront <- gsub("0", "Not waterfront", df$waterfront)
df$waterfront <- gsub("1", "Waterfront", df$waterfront)

ggplot(df, aes(price)) + 
  geom_histogram(aes(fill=waterfront), color = "black") +
  ggtitle("Distribution of price, by waterfront status")

#keep all waterfront:
df$waterfront <- as.factor(df$waterfront)
waterfront <- df %>% 
  filter(waterfront == "Waterfront")

#sample 10% of non waterfront
set.seed(2020)
not.waterfront <- df %>% 
  filter(waterfront == "Not waterfront")
sample.nw <- sample_n(not.waterfront, size = ceiling(.1*nrow(df)))

#create new df to include only what we're interested in
subset.df <- df %>% 
  dplyr::select(price:condition, effective_age)

#create testing and training data
index <- createDataPartition(subset.df$waterfront, p = .70, list = FALSE)
train <- subset.df[index, ]
test <- subset.df[-index, ]

#build logit model; need to put dummy var back
df$waterfront <- gsub("Not waterfront", "0", subset.df$waterfront)
df$waterfront <- gsub("Waterfront", "1", subset.df$waterfront)

##
df$waterfront <- as.factor(df$waterfront)
#need to look @ weights here
logit.model <- glm(as.factor(waterfront) ~ ., weights = 10, family = binomial(), train)

#suggests some of these variables are significant
summary(logit.model)

pred.prob <- predict(logit.model, test, type = "response")

# Converting from probability to actual output
train$pred.class <- ifelse(logit.model$fitted.values >= 0.5, "Waterfront", "Not Waterfront")

test$pred.class <- ifelse(pred.prob >= 0.5, "Waterfront", "Not Waterfront")
 
# Generating the classification table
ctab_train <- table(train$waterfront, train$pred.class)
ctab_train

ctab_test <- table(test$waterfront, test$pred.class)
ctab_test

#check accuracy on training; add diagonals
accuracy.train <- (ctab_train[1,1] + ctab_train[2,2])/(sum(ctab_train))
cat("The accuracy on the training data is:", accuracy.train*100, "%")

#now check testing accuracy
accuracy.test <- (ctab_test[1,1] + ctab_test[2,2])/(sum(ctab_test))
cat("The accuracy on the testing data is:", accuracy.test*100, "%")
```

Due to high imbalance, we must assign weights.

# References
https://www.r-bloggers.com/outlier-detection-and-treatment-with-r/
http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/
https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r
https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/
https://rstatisticsblog.com/data-science-in-action/machine-learning/binary-logistic-regression-with-r/
https://www.r-bloggers.com/dealing-with-unbalanced-data-in-machine-learning/



